{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv OpenAI fastapi uvicorn nest_asyncio pydantic tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"GILAS_API_KEY\"),\n",
    "    base_url=\"https://api.gilas.io/v1/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"ft:gpt-4o-mini:xxxx:zzzzz\",\n",
    "          \"gpt-4o-mini\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 4095\n",
    "\n",
    "def get_completion(instruction, prompt, model):\n",
    "    messages = [{\"role\": \"system\", \"content\": instruction},\n",
    "                {\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = f\"\"\"\n",
    "You are an intelligent translator specializing in translating technical texts from English to Farsi. Please follow these steps to translate the provided English texts to Farsi:\n",
    "1. Thoroughly understand the given text.\n",
    "2. Translate the content into fluent Farsi, preserving the original structure and flow.\n",
    "3. Tailor the translation for software engineers by keeping the technical and programming terms in English, enclosed in backticks (e.g., `GPT-3`).\n",
    "4. You can include additional explanations in Farsi for clarity, if needed.\n",
    "5. If you encounter a block of text enclosed in triple backticks, do not translate it; keep it as it is in your translation.\n",
    "6. Review your translation to ensure you followed the steps. If the translated text is not fluent Farsi, revise it. \n",
    "7. Your output should only include the very final version and formatted translation. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_test_data():\n",
    "    file_path = \"test-data.txt\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    if not hasattr(get_next_test_data, \"current_index\"):\n",
    "        get_next_test_data.current_index = 0\n",
    "\n",
    "    while get_next_test_data.current_index < len(lines):\n",
    "        next_line = lines[get_next_test_data.current_index].strip()\n",
    "        get_next_test_data.current_index += 1\n",
    "        if next_line:\n",
    "            return next_line\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_html_table(prompt, model_responses):\n",
    "    # Initialize the table with headers if it's the first call\n",
    "    if not hasattr(generate_html_table, 'header_added'):\n",
    "        generate_html_table.html_content = f\"<table>\\n<tr><th>English Text</th>\"\n",
    "        for model_name in model_responses:\n",
    "            generate_html_table.html_content += f\"<th>{model_name}</th>\"\n",
    "        generate_html_table.html_content += \"</tr>\\n\"\n",
    "        generate_html_table.header_added = True\n",
    "    else:\n",
    "        # Remove the closing </table> tag if it exists\n",
    "        if generate_html_table.html_content.endswith(\"</table>\"):\n",
    "            generate_html_table.html_content = generate_html_table.html_content[:-8]\n",
    "\n",
    "\n",
    "    # Append new row to the html content\n",
    "    generate_html_table.html_content += f\"<tr><td>{prompt}</td>\"\n",
    "    for response in model_responses.values():\n",
    "        generate_html_table.html_content += f\"\"\"<td><input type=\"checkbox\"><label dir=\"rtl\">{response}</label></td>\"\"\"\n",
    "    generate_html_table.html_content += \"</tr>\\n\"\n",
    "\n",
    "    # Re-add the closing </table> tag\n",
    "    generate_html_table.html_content += \"</table>\"\n",
    "\n",
    "    return generate_html_table.html_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def gen_prompt_response(sample):\n",
    "    # Create a list of tasks to run synchronous functions in separate threads\n",
    "    tasks = [asyncio.to_thread(get_completion, instruction, sample, model_name) for model_name in models]\n",
    "    \n",
    "    # Wait for all tasks to complete\n",
    "    responses = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Pair model names with their respective responses\n",
    "    model_responses = {model_name: response for model_name, response in zip(models, responses)}\n",
    "    \n",
    "    # Generate the HTML table with the results\n",
    "    return generate_html_table(sample, model_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from fastapi.responses import HTMLResponse\n",
    "import uvicorn\n",
    "import nest_asyncio\n",
    "from typing import List, Dict\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\", response_class=HTMLResponse)\n",
    "async def prompt_endpoint():\n",
    "    html_table = \"\"\n",
    "    sample = get_next_test_data()\n",
    "    while sample:\n",
    "        html_table = await gen_prompt_response(sample)\n",
    "        sample = get_next_test_data()\n",
    "\n",
    "    \n",
    "    html_content = f\"\"\"<html>\n",
    "    <head><title>Prompt Results</title>\n",
    "    </head>\n",
    "    \n",
    "    <body> \n",
    "    {html_table} \n",
    "    </body></html>\"\"\"\n",
    "\n",
    "\n",
    "    return html_content\n",
    "\n",
    "  \n",
    "# If using asyncio in Jupyter, apply the following:\n",
    "nest_asyncio.apply()\n",
    "\n",
    "uvicorn.run(app, host=\"127.0.0.1\", port=8000, log_level=\"info\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
